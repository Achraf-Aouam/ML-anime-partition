{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video partitionning using mobilenetV2 CNN and Bi-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SIZE = 160  # Resize frames to 160x160\n",
    "FPS = 4  # Sample 8 frames per second\n",
    "SEQUENCE_LENGTH = 120  # 30 seconds * 4 fps\n",
    "BATCH_SIZE = 16  # Will be adjusted based on memory\n",
    "NUM_CLASSES = 3  # intro, regular, outro\n",
    "FEATURE_DIM = 512  # Reduced feature dimension\n",
    "HIDDEN_DIM = 512  # LSTM hidden dimension\n",
    "ACCUMULATION_STEPS = 4  # Effective batch size of 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, video_dir, json_dir, clip_length=30, mode='train', train_split=0.7, val_split=0.15):\n",
    "        \"\"\"\n",
    "        Dataset for loading anime episodes and their annotations.\n",
    "        \n",
    "        Args:\n",
    "            video_dir: Directory containing video files\n",
    "            json_dir: Directory containing annotation JSON files\n",
    "            clip_length: Length of clips in seconds\n",
    "            mode: 'train', 'val', or 'test'\n",
    "            train_split: Proportion of data for training\n",
    "            val_split: Proportion of data for validation\n",
    "        \"\"\"\n",
    "        self.video_dir = Path(video_dir)\n",
    "        self.json_dir = Path(json_dir)\n",
    "        self.clip_length = clip_length\n",
    "        self.frame_interval = 1.0 / FPS\n",
    "\n",
    "         # Debug print\n",
    "        print(f\"Initializing dataset with video_dir: {video_dir}, json_dir: {json_dir}\")\n",
    "        \n",
    "        # Get all video files\n",
    "        self.video_files = sorted(list(self.video_dir.glob('*.mp4')))\n",
    "        print(f\"Found {len(self.video_files)} video files\")\n",
    "        \n",
    "        # Split data\n",
    "        n_videos = len(self.video_files)\n",
    "        train_idx = int(n_videos * train_split)\n",
    "        val_idx = int(n_videos * (train_split + val_split))\n",
    "        print(f\"Total videos: {n_videos}, Train idx: {train_idx}, Val idx: {val_idx}\")\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.video_files = self.video_files[:train_idx]\n",
    "        elif mode == 'val':\n",
    "            self.video_files = self.video_files[train_idx:val_idx]\n",
    "        else:  # test\n",
    "            self.video_files = self.video_files[val_idx:]\n",
    "            \n",
    "        # Load annotations with debug prints\n",
    "        self.annotations = {}\n",
    "        for video_file in self.video_files:\n",
    "            json_file = self.json_dir / f\"{video_file.stem}.json\"\n",
    "            print(f\"Loading annotation: {json_file}\")\n",
    "            if not json_file.exists():\n",
    "                print(f\"Warning: Missing annotation file for {video_file.stem}\")\n",
    "                continue\n",
    "            with open(json_file, 'r') as f:\n",
    "                self.annotations[video_file.stem] = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded {len(self.annotations)} annotations\")\n",
    "\n",
    "        # Print first annotation as example\n",
    "        if self.annotations:\n",
    "            first_key = next(iter(self.annotations))\n",
    "            print(f\"Sample annotation: {self.annotations[first_key]}\")\n",
    "                \n",
    "        # Create clips with balanced sampling\n",
    "        self.clips = self._create_clips()\n",
    "        print(f\"Created {len(self.clips)} clips\")\n",
    "       \n",
    "         # Updated transforms with augmentation\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "            transforms.RandomHorizontalFlip(p=0.3),  # New\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),  # New\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ]) if mode == 'train' else transforms.Compose([  # Different transform for val/test\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def _create_clips(self):\n",
    "        \"\"\"\n",
    "        Create clips with balanced sampling between intro/regular/outro content.\n",
    "        Returns list of (video_path, start_time) tuples.\n",
    "        \"\"\"\n",
    "        clips = []\n",
    "        intro_clips = []\n",
    "        regular_clips = []\n",
    "        outro_clips = []\n",
    "        \n",
    "        for video_file in self.video_files:\n",
    "            if video_file.stem not in self.annotations:\n",
    "                print(f\"Skipping {video_file.stem} - no annotation\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            ann = self.annotations[video_file.stem]\n",
    "\n",
    "\n",
    "\n",
    "            try:\n",
    "                video_length = self._get_video_length(video_file)\n",
    "                print(f\"Video {video_file.stem} length: {video_length}\")\n",
    "\n",
    "                # Add debug prints for intro/outro times\n",
    "                print(f\"Intro: {ann.get('intro_start', 'None')} to {ann.get('intro_end', 'None')}\")\n",
    "                print(f\"Outro: {ann.get('outro_start', 'None')} to {ann.get('outro_end', 'None')}\")\n",
    "            \n",
    "                # Add intro clips\n",
    "                if 'intro_start' in ann and 'intro_end' in ann:\n",
    "                    intro_start = ann['intro_start']\n",
    "                    intro_end = ann['intro_end']\n",
    "                    # Sample clips that contain intro\n",
    "                    possible_starts = np.arange(\n",
    "                        max(0, intro_start - self.clip_length + 30),\n",
    "                        min(intro_end, video_length - self.clip_length),\n",
    "                        30  # Sample every 30 seconds\n",
    "                    )\n",
    "                    for start in possible_starts:\n",
    "                        intro_clips.append((video_file, start))\n",
    "                \n",
    "                # Add outro clips\n",
    "                if 'outro_start' in ann and 'outro_end' in ann:\n",
    "                    outro_start = ann['outro_start']\n",
    "                    outro_end = ann['outro_end']\n",
    "                    possible_starts = np.arange(\n",
    "                        max(0, outro_start - self.clip_length + 30),\n",
    "                        min(outro_end, video_length - self.clip_length),\n",
    "                        30\n",
    "                    )\n",
    "                    for start in possible_starts:\n",
    "                        outro_clips.append((video_file, start))\n",
    "                \n",
    "                # Add regular clips\n",
    "                regular_regions = self._get_regular_regions(ann, video_length)\n",
    "                for start, end in regular_regions:\n",
    "                    possible_starts = np.arange(\n",
    "                        start,\n",
    "                        end - self.clip_length,\n",
    "                        self.clip_length\n",
    "                    )\n",
    "                    for start in possible_starts:\n",
    "                        regular_clips.append((video_file, start))\n",
    "            \n",
    "                # Balance dataset by upsampling intro and outro clips\n",
    "                n_regular = len(regular_clips)\n",
    "                intro_clips = intro_clips * (n_regular // len(intro_clips) + 1)\n",
    "                outro_clips = outro_clips * (n_regular // len(outro_clips) + 1)\n",
    "                \n",
    "                # Combine and shuffle\n",
    "                clips = intro_clips[:n_regular//3] + outro_clips[:n_regular//3] + regular_clips[:n_regular//3]\n",
    "                random.shuffle(clips)\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_file.stem}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        \n",
    "         # Print clip counts\n",
    "        print(f\"Found clips - Intro: {len(intro_clips)}, Regular: {len(regular_clips)}, Outro: {len(outro_clips)}\")\n",
    "\n",
    "        return clips\n",
    "    \n",
    "    def _get_video_length(self, video_path):\n",
    "        \"\"\"Get video length in seconds with debug info.\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Failed to open video: {video_path}\")\n",
    "            return 0\n",
    "            \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "        \n",
    "        if fps == 0:\n",
    "            print(f\"Warning: Zero FPS for video {video_path}\")\n",
    "            return 0\n",
    "            \n",
    "        length = frame_count / fps\n",
    "        print(f\"Video {video_path}: {frame_count} frames at {fps} FPS = {length} seconds\")\n",
    "        return length\n",
    "    \n",
    "    def _get_regular_regions(self, ann, video_length):\n",
    "        \"\"\"Get time regions that are neither intro nor outro.\"\"\"\n",
    "        regions = []\n",
    "        intro_end = ann.get('intro_end', 0)\n",
    "        outro_start = ann.get('outro_start', video_length)\n",
    "        \n",
    "        if intro_end < outro_start:\n",
    "            regions.append((intro_end, outro_start))\n",
    "            \n",
    "        return regions\n",
    "    \n",
    "    def _get_frame_labels(self, video_file, start_time, n_frames):\n",
    "        \"\"\"Generate frame-level labels.\"\"\"\n",
    "        ann = self.annotations[video_file.stem]\n",
    "        times = np.linspace(start_time, start_time + self.clip_length, n_frames)\n",
    "        labels = np.zeros((n_frames, NUM_CLASSES))\n",
    "        \n",
    "        for i, t in enumerate(times):\n",
    "            if ann['intro_start'] <= t <= ann['intro_end']:\n",
    "                labels[i, 0] = 1  # intro\n",
    "            elif ann.get('outro_start', float('inf')) <= t <= ann.get('outro_end', float('inf')):\n",
    "                labels[i, 2] = 1  # outro\n",
    "            else:\n",
    "                labels[i, 1] = 1  # regular\n",
    "                \n",
    "        return labels\n",
    "    \n",
    "    def _sample_frames(self, video_path, start_time):\n",
    "        \"\"\"Sample frames from video at specified FPS.\"\"\"\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        \n",
    "        # Set starting position\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)\n",
    "        \n",
    "        # Sample frames\n",
    "        for _ in range(SEQUENCE_LENGTH):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Convert BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "            \n",
    "            # Skip frames to maintain desired FPS\n",
    "            for _ in range(int(cap.get(cv2.CAP_PROP_FPS) / FPS) - 1):\n",
    "                cap.read()\n",
    "                \n",
    "        cap.release()\n",
    "        \n",
    "        # Pad sequence if needed\n",
    "        while len(frames) < SEQUENCE_LENGTH:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "            \n",
    "        return torch.stack(frames)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.clips)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_file, start_time = self.clips[idx]\n",
    "        frames = self._sample_frames(video_file, start_time)\n",
    "        labels = self._get_frame_labels(video_file, start_time, SEQUENCE_LENGTH)\n",
    "        return frames, torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_labels(dataset):\n",
    "    \"\"\"Verify label distribution and transitions in the dataset.\"\"\"\n",
    "    transitions = 0\n",
    "    total_frames = 0\n",
    "    label_counts = [0, 0, 0]\n",
    "    \n",
    "    for i in range(min(100, len(dataset))):  # Check first 100 items\n",
    "        _, labels = dataset[i]\n",
    "        total_frames += labels.shape[0]\n",
    "        \n",
    "        # Count label distributions\n",
    "        for j in range(3):\n",
    "            label_counts[j] += (labels[:, j] == 1).sum().item()\n",
    "            \n",
    "        # Count transitions\n",
    "        for t in range(1, labels.shape[0]):\n",
    "            if not torch.equal(labels[t], labels[t-1]):\n",
    "                transitions += 1\n",
    "    \n",
    "    print(f\"\\nLabel Distribution Analysis:\")\n",
    "    print(f\"Total frames checked: {total_frames}\")\n",
    "    print(f\"Label counts: Intro: {label_counts[0]}, Regular: {label_counts[1]}, Outro: {label_counts[2]}\")\n",
    "    print(f\"Label percentages: Intro: {label_counts[0]/total_frames*100:.1f}%, \"\n",
    "          f\"Regular: {label_counts[1]/total_frames*100:.1f}%, \"\n",
    "          f\"Outro: {label_counts[2]/total_frames*100:.1f}%\")\n",
    "    print(f\"Number of transitions: {transitions}\")\n",
    "    print(f\"Transitions per sequence: {transitions/100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model defenition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class AnimeClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # Feature extraction using MobileNetV2 (still efficient but full version)\n",
    "#         mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        \n",
    "#         # Adjust batch norm momentum for better training\n",
    "#         for m in mobilenet.modules():\n",
    "#             if isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.momentum = 0.1\n",
    "#                 m.eps = 1e-3\n",
    "#         self.features = nn.Sequential(*list(mobilenet.children())[:-1])\n",
    "        \n",
    "#         # Feature dimension reduction\n",
    "#         self.feature_reduction = nn.Sequential(\n",
    "#             nn.AdaptiveAvgPool2d(1),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(1280, FEATURE_DIM),\n",
    "#             nn.BatchNorm1d(FEATURE_DIM),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3)\n",
    "#         )\n",
    "        \n",
    "#         # Bidirectional LSTM layers\n",
    "#         self.lstm1 = nn.LSTM(FEATURE_DIM, HIDDEN_DIM, bidirectional=True, \n",
    "#                             batch_first=True, num_layers=2, dropout=0.3)\n",
    "#         self.lstm2 = nn.LSTM(HIDDEN_DIM * 2, HIDDEN_DIM, bidirectional=True, \n",
    "#                             batch_first=True, num_layers=2, dropout=0.3)\n",
    "        \n",
    "#         # Residual connection\n",
    "#         self.residual_projection = nn.Sequential(\n",
    "#             nn.Linear(FEATURE_DIM, HIDDEN_DIM * 2),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "        \n",
    "#         # Classification head (outputs logits for BCE with logits)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(HIDDEN_DIM * 2, HIDDEN_DIM),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(HIDDEN_DIM, NUM_CLASSES)\n",
    "#         )\n",
    "#         self._initialize_weights()\n",
    "\n",
    "#     def _initialize_weights(self):\n",
    "#         # Initialize LSTM weights\n",
    "#         for lstm in [self.lstm1, self.lstm2]:\n",
    "#             for name, param in lstm.named_parameters():\n",
    "#                 if 'weight_ih' in name:\n",
    "#                     nn.init.xavier_uniform_(param.data)\n",
    "#                 elif 'weight_hh' in name:\n",
    "#                     nn.init.orthogonal_(param.data)\n",
    "#                 elif 'bias' in name:\n",
    "#                     param.data.fill_(0)\n",
    "        \n",
    "#         # Initialize linear layers\n",
    "#         for m in self.feature_reduction.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "#         for m in self.residual_projection.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "#         for m in self.classifier.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         batch_size, seq_len, c, h, w = x.size()\n",
    "        \n",
    "#         # Process each frame through CNN\n",
    "#         x = x.view(batch_size * seq_len, c, h, w)\n",
    "#         features = self.features(x)\n",
    "#         features = self.feature_reduction(features)\n",
    "#         features = features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "#         # Save for residual connection\n",
    "#         residual = self.residual_projection(features)\n",
    "        \n",
    "#         # Process through LSTM layers\n",
    "#         lstm_out1, _ = self.lstm1(features)\n",
    "#         lstm_out2, _ = self.lstm2(lstm_out1)\n",
    "        \n",
    "#         # Add residual connection\n",
    "#         lstm_out = lstm_out2 + residual\n",
    "        \n",
    "#         # Classification\n",
    "#         return self.classifier(lstm_out)  # Returns logits\n",
    "class AnimeClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use MobileNetV2 but freeze early layers\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        # Freeze first 10 layers\n",
    "        for i, param in enumerate(mobilenet.features[:5].parameters()):\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.features = nn.Sequential(*list(mobilenet.children())[:-1])\n",
    "        \n",
    "        # Wider feature reduction\n",
    "        self.feature_reduction = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, FEATURE_DIM),\n",
    "            nn.LayerNorm(FEATURE_DIM),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(FEATURE_DIM, FEATURE_DIM),\n",
    "            nn.LayerNorm(FEATURE_DIM),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Single LSTM layer with higher capacity\n",
    "        self.lstm = nn.LSTM(FEATURE_DIM, HIDDEN_DIM, \n",
    "                           bidirectional=True, \n",
    "                           batch_first=True,\n",
    "                           num_layers=2,  # Single layer but higher dimension\n",
    "                           dropout=0.2)\n",
    "        \n",
    "        # # Simplified classification head\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Linear(HIDDEN_DIM * 2, HIDDEN_DIM),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(0.2),\n",
    "        #     nn.Linear(HIDDEN_DIM, NUM_CLASSES)\n",
    "        # )\n",
    "        # Wider classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_DIM * 2, HIDDEN_DIM * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(HIDDEN_DIM * 2, HIDDEN_DIM),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(HIDDEN_DIM, NUM_CLASSES)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        \n",
    "        # Process each frame through CNN\n",
    "        x = x.view(batch_size * seq_len, c, h, w)\n",
    "        features = self.features(x)\n",
    "        features = self.feature_reduction(features)\n",
    "        features = features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Process through LSTM layer\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(lstm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function + training epoch + validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_consistency_loss(predictions, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Calculate temporal consistency loss to penalize rapid changes in predictions.\n",
    "    \"\"\"\n",
    "    temp_loss = torch.mean(torch.abs(predictions[:, 1:] - predictions[:, :-1]))\n",
    "    return alpha * temp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc='Training')\n",
    "    \n",
    "    for frames, labels in progress_bar:\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            logits = model(frames)\n",
    "            loss = criterion(torch.sigmoid(logits), labels)  # Apply sigmoid here\n",
    "            temp_loss = temporal_consistency_loss(torch.sigmoid(logits))\n",
    "            loss = loss + temp_loss\n",
    "        \n",
    "        # scaler.scale(loss).backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        # Add gradient clipping\n",
    "        if scaler.is_enabled():  # Check if we're using mixed precision\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(val_loader, desc='Validation'):\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "            \n",
    "            predictions = model(frames)\n",
    "            bce_loss = criterion(predictions, labels)\n",
    "            temp_loss = temporal_consistency_loss(predictions)\n",
    "            loss = bce_loss + temp_loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.append(predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, 0)\n",
    "    all_labels = torch.cat(all_labels, 0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = ((all_preds.argmax(dim=-1) == all_labels.argmax(dim=-1)).float().mean()).item()\n",
    "    \n",
    "    return total_loss / len(val_loader), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=15, patience=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Use BCEWithLogitsLoss instead of BCELoss for better numeric stability\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-3, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "   # Better learning rate scheduling with warmup\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=2e-2,  # Peak learning rate\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.2,  # 10% warmup\n",
    "        div_factor=10,  # Initial lr = max_lr/10\n",
    "        final_div_factor=100,  # Final lr = initial_lr/100\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'epoch_times': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    best_model_path = f'best_model_{current_time}.pth'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "            val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "            \n",
    "            scheduler.step()\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            # Save periodic checkpoint (every 5 epochs)\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                checkpoint_path = f'checkpoint_epoch_{epoch+1}_{current_time}.pth'\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'history': history\n",
    "                }, checkpoint_path)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                }, best_model_path)\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "\n",
    "             # Record history\n",
    "            epoch_time = time.time() - start_time\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_accuracy'].append(val_accuracy)\n",
    "            history['epoch_times'].append(epoch_time)\n",
    "            history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "            print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.4f}, LR: {current_lr:.6f}')\n",
    "            print(f'Epoch time: {epoch_time:.2f}s')\n",
    "            \n",
    "                \n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'\\nEarly stopping after {epoch + 1} epochs')\n",
    "                break\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "        \n",
    "    finally:\n",
    "        # Always try to save final state\n",
    "        try:\n",
    "            final_path = f'final_model_{current_time}.pth'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'history': history\n",
    "            }, final_path)\n",
    "            print(f\"\\nFinal model saved to {final_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving final model: {str(e)}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training metrics over time.\n",
    "    Shows loss curves and validation accuracy.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Validation Accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot epoch times\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['epoch_times'], label='Epoch Time')\n",
    "    plt.title('Training Time per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(model, video_path, json_path=None, window_size=60, overlap=30):\n",
    "    \"\"\"\n",
    "    Predict intro/outro segments for an entire video using overlapping windows.\n",
    "    Returns frame-level predictions and optionally compares with ground truth.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = 'cpu'\n",
    "    model.eval()\n",
    "    \n",
    "    # Video properties\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / video_fps\n",
    "    cap.release()\n",
    "    \n",
    "    # Calculate number of windows with overlap\n",
    "    stride = window_size - overlap\n",
    "    n_windows = int(np.ceil((duration - window_size) / stride)) + 1\n",
    "    \n",
    "    # Initialize arrays for predictions\n",
    "    all_predictions = np.zeros((int(duration * FPS), NUM_CLASSES))\n",
    "    counts = np.zeros(int(duration * FPS))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Process each window\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(n_windows), desc='Processing video'):\n",
    "            start_time = i * stride\n",
    "            \n",
    "            # Sample frames for this window\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)\n",
    "            \n",
    "            frames = []\n",
    "            for _ in range(SEQUENCE_LENGTH):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = transform(frame)\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "            \n",
    "            # If we don't have enough frames, pad with zeros\n",
    "            while len(frames) < SEQUENCE_LENGTH:\n",
    "                frames.append(torch.zeros_like(frames[0]))\n",
    "            \n",
    "            # Make prediction\n",
    "            frames_tensor = torch.stack(frames).unsqueeze(0).to(device)\n",
    "            predictions = model(frames_tensor).cpu().numpy()[0]\n",
    "            \n",
    "            # Add predictions to the appropriate time slots\n",
    "            start_idx = int(start_time * FPS)\n",
    "            end_idx = start_idx + predictions.shape[0]\n",
    "            all_predictions[start_idx:end_idx] += predictions\n",
    "            counts[start_idx:end_idx] += 1\n",
    "    \n",
    "    # Average overlapping predictions\n",
    "    mask = counts > 0\n",
    "    all_predictions[mask] /= counts[mask, np.newaxis]\n",
    "    \n",
    "    # Load ground truth if available\n",
    "    if json_path:\n",
    "        with open(json_path, 'r') as f:\n",
    "            gt = json.load(f)\n",
    "            \n",
    "        gt_labels = np.zeros((int(duration * FPS), NUM_CLASSES))\n",
    "        times = np.arange(0, duration, 1/FPS)\n",
    "        \n",
    "        for i, t in enumerate(times):\n",
    "            if gt['intro_start'] <= t <= gt['intro_end']:\n",
    "                gt_labels[i, 0] = 1\n",
    "            elif gt.get('outro_start', float('inf')) <= t <= gt.get('outro_end', float('inf')):\n",
    "                gt_labels[i, 2] = 1\n",
    "            else:\n",
    "                gt_labels[i, 1] = 1\n",
    "    else:\n",
    "        gt_labels = None\n",
    "    \n",
    "    return all_predictions, gt_labels, times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizes model predictions against ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(predictions, ground_truth, times):\n",
    "    \"\"\"\n",
    "    Visualize predictions against ground truth if available.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.plot(times, predictions[:, 0], 'b-', label='Intro Pred', alpha=0.7)\n",
    "    plt.plot(times, predictions[:, 2], 'r-', label='Outro Pred', alpha=0.7)\n",
    "    \n",
    "    if ground_truth is not None:\n",
    "        # Plot ground truth\n",
    "        plt.plot(times, ground_truth[:, 0], 'b--', label='Intro GT')\n",
    "        plt.plot(times, ground_truth[:, 2], 'r--', label='Outro GT')\n",
    "    \n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Intro/Outro Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset with video_dir: /teamspace/studios/this_studio/100anime, json_dir: /teamspace/studios/this_studio/100 anime\n",
      "Found 103 video files\n",
      "Total videos: 103, Train idx: 72, Val idx: 87\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/1.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/10.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/100.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/101.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/102.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/103.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/11.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/12.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/13.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/14.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/15.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/16.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/17.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/18.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/19.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/2.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/20.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/21.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/22.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/23.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/24.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/25.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/26.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/27.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/28.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/29.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/3.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/30.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/31.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/32.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/33.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/34.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/35.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/36.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/37.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/38.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/39.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/4.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/40.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/41.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/42.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/43.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/44.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/45.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/46.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/47.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/48.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/49.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/5.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/50.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/51.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/52.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/53.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/54.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/55.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/56.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/57.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/58.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/59.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/6.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/60.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/61.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/62.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/63.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/64.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/65.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/66.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/67.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/68.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/69.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/7.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/70.json\n",
      "Loaded 72 annotations\n",
      "Sample annotation: {'intro_start': 72.98958333333333, 'intro_end': 162.24541666666664, 'outro_start': 1310.2672916666666, 'outro_end': 1401.2331666666666}\n",
      "Video /teamspace/studios/this_studio/100anime/1.mp4: 34286 frames at 23.976023976023978 FPS = 1430.0119166666666 seconds\n",
      "Video 1 length: 1430.0119166666666\n",
      "Intro: 72.98958333333333 to 162.24541666666664\n",
      "Outro: 1310.2672916666666 to 1401.2331666666666\n",
      "Video /teamspace/studios/this_studio/100anime/10.mp4: 34408 frames at 23.976023976023978 FPS = 1435.1003333333333 seconds\n",
      "Video 10 length: 1435.1003333333333\n",
      "Intro: 71.90516666666666 to 160.70220833333332\n",
      "Outro: 1344.968625 to 1435.1003333333333\n",
      "Video /teamspace/studios/this_studio/100anime/100.mp4: 34409 frames at 23.976023976023978 FPS = 1435.1420416666665 seconds\n",
      "Video 100 length: 1435.1420416666665\n",
      "Intro: 0.04170833333333333 to 165.08158333333333\n",
      "Outro: 1395.4774166666666 to 1400.4774166666666\n",
      "Video /teamspace/studios/this_studio/100anime/101.mp4: 35484 frames at 23.976023976023978 FPS = 1479.9785 seconds\n",
      "Video 101 length: 1479.9785\n",
      "Intro: 34.45108333333333 to 128.67020833333333\n",
      "Outro: 1248.5806666666665 to 1291.9156249999999\n",
      "Video /teamspace/studios/this_studio/100anime/102.mp4: 38365 frames at 23.976023976023978 FPS = 1600.1402083333332 seconds\n",
      "Video 102 length: 1600.1402083333332\n",
      "Intro: 0.37537499999999996 to 104.39595833333333\n",
      "Outro: 1414.5381249999998 to 1597.34575\n",
      "Video /teamspace/studios/this_studio/100anime/103.mp4: 72000 frames at 47.952047952047955 FPS = 1501.5 seconds\n",
      "Video 103 length: 1501.5\n",
      "Intro: 0.6464791666666666 to 100.99672916666665\n",
      "Outro: 1349.5565416666666 to 1441.5442708333333\n",
      "Video /teamspace/studios/this_studio/100anime/11.mp4: 34408 frames at 23.976023976023978 FPS = 1435.1003333333333 seconds\n",
      "Video 11 length: 1435.1003333333333\n",
      "Intro: 284.53425 to 375.33329166666664\n",
      "Outro: 1330.329 to 1434.8083749999998\n",
      "Video /teamspace/studios/this_studio/100anime/12.mp4: 68861 frames at 47.952047952047955 FPS = 1436.0387708333333 seconds\n",
      "Video 12 length: 1436.0387708333333\n",
      "Intro: 1.4597916666666666 to 91.54979166666666\n",
      "Outro: 1413.3494374999998 to 1423.6931041666667\n",
      "Video /teamspace/studios/this_studio/100anime/13.mp4: 32909 frames at 23.976023976023978 FPS = 1372.5795416666665 seconds\n",
      "Video 13 length: 1372.5795416666665\n",
      "Intro: 55.88916666666666 to 145.35354166666664\n",
      "Outro: 1279.4031249999998 to 1371.2865833333333\n",
      "Video /teamspace/studios/this_studio/100anime/14.mp4: 35127 frames at 23.976023976023978 FPS = 1465.0886249999999 seconds\n",
      "Video 14 length: 1465.0886249999999\n",
      "Intro: 139.72291666666666 to 216.5079583333333\n",
      "Outro: 1354.9369166666665 to 1445.9444999999998\n",
      "Video /teamspace/studios/this_studio/100anime/15.mp4: 34097 frames at 23.976023976023978 FPS = 1422.1290416666666 seconds\n",
      "Video 15 length: 1422.1290416666666\n",
      "Intro: 288.8302083333333 to 298.0894583333333\n",
      "Outro: 1414.2461666666666 to 1421.2531666666666\n",
      "Video /teamspace/studios/this_studio/100anime/16.mp4: 34383 frames at 23.976023976023978 FPS = 1434.057625 seconds\n",
      "Video 16 length: 1434.057625\n",
      "Intro: 134.46766666666664 to 224.39083333333332\n",
      "Outro: 1278.5272499999999 to 1419.9602083333332\n",
      "Video /teamspace/studios/this_studio/100anime/17.mp4: 34307 frames at 23.976023976023978 FPS = 1430.8877916666665 seconds\n",
      "Video 17 length: 1430.8877916666665\n",
      "Intro: 0.9175833333333333 to 100.93416666666666\n",
      "Outro: 1210.1672916666666 to 1396.14475\n",
      "Video /teamspace/studios/this_studio/100anime/18.mp4: 34166 frames at 23.976023976023978 FPS = 1425.0069166666665 seconds\n",
      "Video 18 length: 1425.0069166666665\n",
      "Intro: 93.42666666666666 to 209.66779166666666\n",
      "Outro: 1319.9853333333333 to 1399.64825\n",
      "Video /teamspace/studios/this_studio/100anime/19.mp4: 34767 frames at 23.976023976023978 FPS = 1450.073625 seconds\n",
      "Video 19 length: 1450.073625\n",
      "Intro: 1.25125 to 93.13470833333332\n",
      "Outro: 1346.6369583333333 to 1435.4757083333332\n",
      "Video /teamspace/studios/this_studio/100anime/2.mp4: 34096 frames at 23.976023976023978 FPS = 1422.0873333333332 seconds\n",
      "Video 2 length: 1422.0873333333332\n",
      "Intro: 43.043 to 133.92545833333332\n",
      "Outro: 1395.7276666666667 to 1420.2521666666667\n",
      "Video /teamspace/studios/this_studio/100anime/20.mp4: 34525 frames at 23.976023976023978 FPS = 1439.9802083333332 seconds\n",
      "Video 20 length: 1439.9802083333332\n",
      "Intro: 52.385666666666665 to 142.18370833333333\n",
      "Outro: 1368.367 to 1439.1043333333332\n",
      "Video /teamspace/studios/this_studio/100anime/21.mp4: 34527 frames at 23.976023976023978 FPS = 1440.063625 seconds\n",
      "Video 21 length: 1440.063625\n",
      "Intro: 51.176125 to 141.72491666666664\n",
      "Outro: 1346.1364583333332 to 1439.0209166666666\n",
      "Video /teamspace/studios/this_studio/100anime/22.mp4: 37285 frames at 23.976023976023978 FPS = 1555.0952083333332 seconds\n",
      "Video 22 length: 1555.0952083333332\n",
      "Intro: 120.20341666666666 to 208.7085\n",
      "Outro: 1454.327875 to 1554.3027499999998\n",
      "Video /teamspace/studios/this_studio/100anime/23.mp4: 35581 frames at 23.976023976023978 FPS = 1484.0242083333333 seconds\n",
      "Video 23 length: 1484.0242083333333\n",
      "Intro: 1.0427083333333333 to 112.98787499999999\n",
      "Outro: 1394.7266666666665 to 1483.5237083333332\n",
      "Video /teamspace/studios/this_studio/100anime/24.mp4: 34562 frames at 23.976023976023978 FPS = 1441.5234166666667 seconds\n",
      "Video 24 length: 1441.5234166666667\n",
      "Intro: 138.72191666666666 to 227.81091666666666\n",
      "Outro: 1336.209875 to 1425.9662083333333\n",
      "Video /teamspace/studios/this_studio/100anime/25.mp4: 34338 frames at 23.976023976023978 FPS = 1432.18075 seconds\n",
      "Video 25 length: 1432.18075\n",
      "Intro: 118.45166666666665 to 208.49995833333332\n",
      "Outro: 1231.0214583333332 to 1320.5275416666666\n",
      "Video /teamspace/studios/this_studio/100anime/26.mp4: 34494 frames at 23.976023976023978 FPS = 1438.68725 seconds\n",
      "Video 26 length: 1438.68725\n",
      "Intro: 35.49379166666667 to 129.33754166666665\n",
      "Outro: 1296.5869583333333 to 1394.6015416666667\n",
      "Video /teamspace/studios/this_studio/100anime/27.mp4: 33323 frames at 23.976023976023978 FPS = 1389.8467916666666 seconds\n",
      "Video 27 length: 1389.8467916666666\n",
      "Intro: 1.4180833333333331 to 90.17341666666665\n",
      "Outro: 1265.1388749999999 to 1354.60325\n",
      "Video /teamspace/studios/this_studio/100anime/28.mp4: 34855 frames at 23.976023976023978 FPS = 1453.7439583333332 seconds\n",
      "Video 28 length: 1453.7439583333332\n",
      "Intro: 1.8351666666666666 to 12.637625\n",
      "Outro: 1425.8410833333332 to 1451.9922083333333\n",
      "Video /teamspace/studios/this_studio/100anime/29.mp4: 35527 frames at 23.976023976023978 FPS = 1481.7719583333333 seconds\n",
      "Video 29 length: 1481.7719583333333\n",
      "Intro: 1.5014999999999998 to 91.50808333333333\n",
      "Outro: 1355.1454583333332 to 1453.1183333333333\n",
      "Video /teamspace/studios/this_studio/100anime/3.mp4: 34047 frames at 23.976023976023978 FPS = 1420.0436249999998 seconds\n",
      "Video 3 length: 1420.0436249999998\n",
      "Intro: 134.759625 to 225.80891666666665\n",
      "Outro: 1413.0783333333331 to 1414.6215416666666\n",
      "Video /teamspace/studios/this_studio/100anime/30.mp4: 34382 frames at 23.976023976023978 FPS = 1434.0159166666665 seconds\n",
      "Video 30 length: 1434.0159166666665\n",
      "Intro: 129.33754166666665 to 219.17729166666666\n",
      "Outro: 1315.6476666666665 to 1404.1944583333332\n",
      "Video /teamspace/studios/this_studio/100anime/31.mp4: 34386 frames at 23.976023976023978 FPS = 1434.18275 seconds\n",
      "Video 31 length: 1434.18275\n",
      "Intro: 151.48466666666667 to 240.4068333333333\n",
      "Outro: 1275.8579166666666 to 1433.1400416666665\n",
      "Video /teamspace/studios/this_studio/100anime/32.mp4: 70470 frames at 47.952047952047955 FPS = 1469.5931249999999 seconds\n",
      "Video 32 length: 1469.5931249999999\n",
      "Intro: 0.37537499999999996 to 100.74647916666666\n",
      "Outro: 1320.1104583333333 to 1409.5539791666665\n",
      "Video /teamspace/studios/this_studio/100anime/33.mp4: 34094 frames at 23.976023976023978 FPS = 1422.0039166666666 seconds\n",
      "Video 33 length: 1422.0039166666666\n",
      "Intro: 50.46708333333333 to 122.03858333333332\n",
      "Outro: 1348.8057916666667 to 1421.2114583333332\n",
      "Video /teamspace/studios/this_studio/100anime/34.mp4: 34049 frames at 23.976023976023978 FPS = 1420.1270416666666 seconds\n",
      "Video 34 length: 1420.1270416666666\n",
      "Intro: 23.732041666666664 to 113.40495833333333\n",
      "Outro: 1071.7373333333333 to 1158.3238333333331\n",
      "Video /teamspace/studios/this_studio/100anime/35.mp4: 34527 frames at 23.976023976023978 FPS = 1440.063625 seconds\n",
      "Video 35 length: 1440.063625\n",
      "Intro: 50.71733333333333 to 140.68220833333334\n",
      "Outro: 1351.7670833333332 to 1439.2294583333332\n",
      "Video /teamspace/studios/this_studio/100anime/36.mp4: 34094 frames at 23.976023976023978 FPS = 1422.0039166666666 seconds\n",
      "Video 36 length: 1422.0039166666666\n",
      "Intro: 188.02116666666666 to 262.7625\n",
      "Outro: 1346.1364583333332 to 1421.1697499999998\n",
      "Video /teamspace/studios/this_studio/100anime/37.mp4: 34855 frames at 23.976023976023978 FPS = 1453.7439583333332 seconds\n",
      "Video 37 length: 1453.7439583333332\n",
      "Intro: 1.4180833333333331 to 11.469791666666666\n",
      "Outro: 1429.5948333333333 to 1451.9922083333333\n",
      "Video /teamspace/studios/this_studio/100anime/38.mp4: 35583 frames at 23.976023976023978 FPS = 1484.1076249999999 seconds\n",
      "Video 38 length: 1484.1076249999999\n",
      "Intro: 1.2095416666666665 to 106.02258333333333\n",
      "Outro: 1332.9149166666666 to 1477.851375\n",
      "Video /teamspace/studios/this_studio/100anime/39.mp4: 34048 frames at 23.976023976023978 FPS = 1420.0853333333332 seconds\n",
      "Video 39 length: 1420.0853333333332\n",
      "Intro: 27.861166666666666 to 126.91845833333332\n",
      "Outro: 1334.8335 to 1419.1677499999998\n",
      "Video /teamspace/studios/this_studio/100anime/4.mp4: 35352 frames at 23.976023976023978 FPS = 1474.473 seconds\n",
      "Video 4 length: 1474.473\n",
      "Intro: 91.21612499999999 to 182.14029166666666\n",
      "Outro: 1339.4214166666666 to 1473.22175\n",
      "Video /teamspace/studios/this_studio/100anime/40.mp4: 33985 frames at 23.976023976023978 FPS = 1417.4577083333331 seconds\n",
      "Video 40 length: 1417.4577083333331\n",
      "Intro: 0.9175833333333333 to 7.632624999999999\n",
      "Outro: 1321.2365833333333 to 1411.0763333333332\n",
      "Video /teamspace/studios/this_studio/100anime/41.mp4: 33985 frames at 23.976023976023978 FPS = 1417.4577083333331 seconds\n",
      "Video 41 length: 1417.4577083333331\n",
      "Intro: 1.0844166666666666 to 90.38195833333333\n",
      "Outro: 1325.699375 to 1416.4567083333332\n",
      "Video /teamspace/studios/this_studio/100anime/42.mp4: 34285 frames at 23.976023976023978 FPS = 1429.9702083333332 seconds\n",
      "Video 42 length: 1429.9702083333332\n",
      "Intro: 81.58149999999999 to 171.08758333333333\n",
      "Outro: 1311.3934166666666 to 1400.6075416666665\n",
      "Video /teamspace/studios/this_studio/100anime/43.mp4: 34455 frames at 23.976023976023978 FPS = 1437.0606249999998 seconds\n",
      "Video 43 length: 1437.0606249999998\n",
      "Intro: 120.28683333333332 to 210.41854166666664\n",
      "Outro: 1332.4144166666665 to 1422.2958749999998\n",
      "Video /teamspace/studios/this_studio/100anime/44.mp4: 58396 frames at 24.0 FPS = 2433.1666666666665 seconds\n",
      "Video 44 length: 2433.1666666666665\n",
      "Intro: 218.66666666666666 to 283.25\n",
      "Outro: 2196.6666666666665 to 2432.4583333333335\n",
      "Video /teamspace/studios/this_studio/100anime/45.mp4: 35581 frames at 23.976023976023978 FPS = 1484.0242083333333 seconds\n",
      "Video 45 length: 1484.0242083333333\n",
      "Intro: 1.6683333333333332 to 109.27583333333332\n",
      "Outro: 1394.14275 to 1483.6905416666666\n",
      "Video /teamspace/studios/this_studio/100anime/46.mp4: 34379 frames at 23.976023976023978 FPS = 1433.8907916666665 seconds\n",
      "Video 46 length: 1433.8907916666665\n",
      "Intro: 207.29041666666666 to 297.25529166666666\n",
      "Outro: 1329.5782499999998 to 1432.2224583333332\n",
      "Video /teamspace/studios/this_studio/100anime/47.mp4: 34408 frames at 23.976023976023978 FPS = 1435.1003333333333 seconds\n",
      "Video 47 length: 1435.1003333333333\n",
      "Intro: 111.61149999999999 to 200.57537499999998\n",
      "Outro: 1345.5525416666667 to 1434.2661666666665\n",
      "Video /teamspace/studios/this_studio/100anime/48.mp4: 34338 frames at 23.976023976023978 FPS = 1432.18075 seconds\n",
      "Video 48 length: 1432.18075\n",
      "Intro: 91.42466666666667 to 182.34883333333332\n",
      "Outro: 1343.0500416666666 to 1430.7209583333333\n",
      "Video /teamspace/studios/this_studio/100anime/49.mp4: 37285 frames at 23.976023976023978 FPS = 1555.0952083333332 seconds\n",
      "Video 49 length: 1555.0952083333332\n",
      "Intro: 1.0427083333333333 to 105.43866666666666\n",
      "Outro: 1453.9107916666665 to 1545.6274166666665\n",
      "Video /teamspace/studios/this_studio/100anime/5.mp4: 34047 frames at 23.976023976023978 FPS = 1420.0436249999998 seconds\n",
      "Video 5 length: 1420.0436249999998\n",
      "Intro: 1.0844166666666666 to 91.25783333333332\n",
      "Outro: 1325.7410833333333 to 1419.1260416666667\n",
      "Video /teamspace/studios/this_studio/100anime/50.mp4: 36781 frames at 23.976023976023978 FPS = 1534.0742083333332 seconds\n",
      "Video 50 length: 1534.0742083333332\n",
      "Intro: 76.5765 to 165.37354166666665\n",
      "Outro: 1350.9746249999998 to 1533.2400416666665\n",
      "Video /teamspace/studios/this_studio/100anime/51.mp4: 35240 frames at 23.976023976023978 FPS = 1469.8016666666665 seconds\n",
      "Video 51 length: 1469.8016666666665\n",
      "Intro: 1.5432083333333333 to 102.977875\n",
      "Outro: 1336.7103749999999 to 1468.1750416666666\n",
      "Video /teamspace/studios/this_studio/100anime/52.mp4: 35396 frames at 23.976023976023978 FPS = 1476.3081666666665 seconds\n",
      "Video 52 length: 1476.3081666666665\n",
      "Intro: 56.72333333333333 to 163.49666666666664\n",
      "Outro: 1354.5198333333333 to 1475.348875\n",
      "Video /teamspace/studios/this_studio/100anime/53.mp4: 34528 frames at 23.976023976023978 FPS = 1440.1053333333332 seconds\n",
      "Video 53 length: 1440.1053333333332\n",
      "Intro: 47.79774999999999 to 144.769625\n",
      "Outro: 1350.3072916666665 to 1437.7696666666666\n",
      "Video /teamspace/studios/this_studio/100anime/54.mp4: 34527 frames at 23.976023976023978 FPS = 1440.063625 seconds\n",
      "Video 54 length: 1440.063625\n",
      "Intro: 58.975583333333326 to 151.06758333333332\n",
      "Outro: 1345.9279166666665 to 1439.1877499999998\n",
      "Video /teamspace/studios/this_studio/100anime/55.mp4: 34767 frames at 23.976023976023978 FPS = 1450.073625 seconds\n",
      "Video 55 length: 1450.073625\n",
      "Intro: 19.81145833333333 to 114.15570833333332\n",
      "Outro: 1345.5525416666667 to 1435.6425416666666\n",
      "Video /teamspace/studios/this_studio/100anime/56.mp4: 32071 frames at 23.976023976023978 FPS = 1337.6279583333333 seconds\n",
      "Video 56 length: 1337.6279583333333\n",
      "Intro: 1.9185833333333333 to 91.67491666666666\n",
      "Outro: 1208.6240833333331 to 1308.05675\n",
      "Video /teamspace/studios/this_studio/100anime/57.mp4: 33329 frames at 23.976023976023978 FPS = 1390.0970416666667 seconds\n",
      "Video 57 length: 1390.0970416666667\n",
      "Intro: 0.2919583333333333 to 66.64991666666666\n",
      "Outro: 1290.914625 to 1309.6833749999998\n",
      "Video /teamspace/studios/this_studio/100anime/58.mp4: 35029 frames at 23.976023976023978 FPS = 1461.0012083333331 seconds\n",
      "Video 58 length: 1461.0012083333331\n",
      "Intro: 115.90745833333332 to 210.16829166666665\n",
      "Outro: 1329.453125 to 1460.20875\n",
      "Video /teamspace/studios/this_studio/100anime/59.mp4: 34095 frames at 23.976023976023978 FPS = 1422.045625 seconds\n",
      "Video 59 length: 1422.045625\n",
      "Intro: 151.98516666666666 to 247.33041666666665\n",
      "Outro: 1297.0040416666666 to 1386.4684166666666\n",
      "Video /teamspace/studios/this_studio/100anime/6.mp4: 33985 frames at 23.976023976023978 FPS = 1417.4577083333331 seconds\n",
      "Video 6 length: 1417.4577083333331\n",
      "Intro: 0.37537499999999996 to 90.25683333333333\n",
      "Outro: 1320.444125 to 1411.1180416666666\n",
      "Video /teamspace/studios/this_studio/100anime/60.mp4: 34378 frames at 23.976023976023978 FPS = 1433.8490833333333 seconds\n",
      "Video 60 length: 1433.8490833333333\n",
      "Intro: 65.190125 to 155.02987499999998\n",
      "Outro: 1314.1878749999998 to 1403.9442083333333\n",
      "Video /teamspace/studios/this_studio/100anime/61.mp4: 34431 frames at 23.976023976023978 FPS = 1436.0596249999999 seconds\n",
      "Video 61 length: 1436.0596249999999\n",
      "Intro: 346.42941666666667 to 436.3942916666666\n",
      "Outro: 1336.4601249999998 to 1426.3832916666665\n",
      "Video /teamspace/studios/this_studio/100anime/62.mp4: 34406 frames at 23.976023976023978 FPS = 1435.0169166666665 seconds\n",
      "Video 62 length: 1435.0169166666665\n",
      "Intro: 193.86033333333333 to 283.7835\n",
      "Outro: 1331.9139166666666 to 1434.6832499999998\n",
      "Video /teamspace/studios/this_studio/100anime/63.mp4: 41546 frames at 29.97002997002997 FPS = 1386.2515333333333 seconds\n",
      "Video 63 length: 1386.2515333333333\n",
      "Intro: 119.5194 to 209.27573333333333\n",
      "Outro: 1207.2393666666667 to 1296.3617333333334\n",
      "Video /teamspace/studios/this_studio/100anime/64.mp4: 34554 frames at 23.976023976023978 FPS = 1441.18975 seconds\n",
      "Video 64 length: 1441.18975\n",
      "Intro: 134.09229166666665 to 223.43154166666665\n",
      "Outro: 1335.3757083333333 to 1425.2154583333333\n",
      "Video /teamspace/studios/this_studio/100anime/65.mp4: 32909 frames at 23.976023976023978 FPS = 1372.5795416666665 seconds\n",
      "Video 65 length: 1372.5795416666665\n",
      "Intro: 33.908874999999995 to 126.000875\n",
      "Outro: 1280.2372916666666 to 1371.2031666666667\n",
      "Video /teamspace/studios/this_studio/100anime/66.mp4: 35967 frames at 23.976023976023978 FPS = 1500.123625 seconds\n",
      "Video 66 length: 1500.123625\n",
      "Intro: 1.2095416666666665 to 124.83304166666666\n",
      "Outro: 1391.4317083333333 to 1461.2514583333332\n",
      "Video /teamspace/studios/this_studio/100anime/67.mp4: 34526 frames at 23.976023976023978 FPS = 1440.0219166666666 seconds\n",
      "Video 67 length: 1440.0219166666666\n",
      "Intro: 43.58520833333333 to 137.17870833333333\n",
      "Outro: 1349.6816666666666 to 1397.521125\n",
      "Video /teamspace/studios/this_studio/100anime/68.mp4: 34095 frames at 23.976023976023978 FPS = 1422.045625 seconds\n",
      "Video 68 length: 1422.045625\n",
      "Intro: 216.3828333333333 to 229.89633333333333\n",
      "Outro: 1323.57225 to 1421.1697499999998\n",
      "Video /teamspace/studios/this_studio/100anime/69.mp4: 66400 frames at 47.952047952047955 FPS = 1384.7166666666665 seconds\n",
      "Video 69 length: 1384.7166666666665\n",
      "Intro: 0.14597916666666666 to 102.74847916666666\n",
      "Outro: 1307.6605208333333 to 1368.2001666666665\n",
      "Video /teamspace/studios/this_studio/100anime/7.mp4: 34047 frames at 23.976023976023978 FPS = 1420.0436249999998 seconds\n",
      "Video 7 length: 1420.0436249999998\n",
      "Intro: 109.94316666666666 to 199.53266666666664\n",
      "Outro: 1325.7827916666665 to 1419.0426249999998\n",
      "Video /teamspace/studios/this_studio/100anime/70.mp4: 34285 frames at 23.976023976023978 FPS = 1429.9702083333332 seconds\n",
      "Video 70 length: 1429.9702083333332\n",
      "Intro: 52.302249999999994 to 142.016875\n",
      "Outro: 1293.3754166666665 to 1401.9005\n",
      "Found clips - Intro: 3555, Regular: 2806, Outro: 3275\n",
      "Created 2805 clips\n",
      "Initializing dataset with video_dir: /teamspace/studios/this_studio/100anime, json_dir: /teamspace/studios/this_studio/100 anime\n",
      "Found 103 video files\n",
      "Total videos: 103, Train idx: 72, Val idx: 87\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/71.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/72.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/73.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/74.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/75.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/76.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/77.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/78.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/79.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/8.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/80.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/81.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/82.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/83.json\n",
      "Loading annotation: /teamspace/studios/this_studio/100 anime/84.json\n",
      "Loaded 15 annotations\n",
      "Sample annotation: {'intro_start': 0.25025, 'intro_end': 68.068, 'outro_start': 1294.0010416666667, 'outro_end': 1313.437125}\n",
      "Video /teamspace/studios/this_studio/100anime/71.mp4: 33327 frames at 23.976023976023978 FPS = 1390.0136249999998 seconds\n",
      "Video 71 length: 1390.0136249999998\n",
      "Intro: 0.25025 to 68.068\n",
      "Outro: 1294.0010416666667 to 1313.437125\n",
      "Video /teamspace/studios/this_studio/100anime/72.mp4: 35581 frames at 23.976023976023978 FPS = 1484.0242083333333 seconds\n",
      "Video 72 length: 1484.0242083333333\n",
      "Intro: 18.977291666666666 to 108.94216666666667\n",
      "Outro: 1393.0583333333332 to 1483.2734583333333\n",
      "Video /teamspace/studios/this_studio/100anime/73.mp4: 32877 frames at 23.976023976023978 FPS = 1371.2448749999999 seconds\n",
      "Video 73 length: 1371.2448749999999\n",
      "Intro: 59.30925 to 148.77362499999998\n",
      "Outro: 1275.4408333333333 to 1370.2855833333333\n",
      "Video /teamspace/studios/this_studio/100anime/74.mp4: 34384 frames at 23.976023976023978 FPS = 1434.0993333333333 seconds\n",
      "Video 74 length: 1434.0993333333333\n",
      "Intro: 0.5005 to 94.094\n",
      "Outro: 1314.438125 to 1404.4447083333332\n",
      "Video /teamspace/studios/this_studio/100anime/75.mp4: 34530 frames at 23.976023976023978 FPS = 1440.1887499999998 seconds\n",
      "Video 75 length: 1440.1887499999998\n",
      "Intro: 61.47808333333333 to 151.3178333333333\n",
      "Outro: 1437.1857499999999 to 1439.5214166666665\n",
      "Video /teamspace/studios/this_studio/100anime/76.mp4: 34045 frames at 23.976023976023978 FPS = 1419.9602083333332 seconds\n",
      "Video 76 length: 1419.9602083333332\n",
      "Intro: 404.77937499999996 to 494.869375\n",
      "Outro: 1315.3974166666667 to 1418.9174999999998\n",
      "Video /teamspace/studios/this_studio/100anime/77.mp4: 34525 frames at 23.976023976023978 FPS = 1439.9802083333332 seconds\n",
      "Video 77 length: 1439.9802083333332\n",
      "Intro: 164.83133333333333 to 255.67208333333332\n",
      "Outro: 1345.3857083333332 to 1438.6038333333333\n",
      "Video /teamspace/studios/this_studio/100anime/78.mp4: 37284 frames at 23.976023976023978 FPS = 1555.0535 seconds\n",
      "Video 78 length: 1555.0535\n",
      "Intro: 15.348666666666666 to 109.48437499999999\n",
      "Outro: 1445.0686249999999 to 1545.1686249999998\n",
      "Video /teamspace/studios/this_studio/100anime/79.mp4: 34668 frames at 23.976023976023978 FPS = 1445.9444999999998 seconds\n",
      "Video 79 length: 1445.9444999999998\n",
      "Intro: 197.65579166666666 to 211.7532083333333\n",
      "Outro: 1440.2721666666666 to 1444.1510416666665\n",
      "Video /teamspace/studios/this_studio/100anime/8.mp4: 34528 frames at 23.976023976023978 FPS = 1440.1053333333332 seconds\n",
      "Video 8 length: 1440.1053333333332\n",
      "Intro: 50.71733333333333 to 140.18170833333332\n",
      "Outro: 1417.9582083333332 to 1418.9592083333332\n",
      "Video /teamspace/studios/this_studio/100anime/80.mp4: 34069 frames at 23.976023976023978 FPS = 1420.9612083333332 seconds\n",
      "Video 80 length: 1420.9612083333332\n",
      "Intro: 348.3897083333333 to 437.89579166666664\n",
      "Outro: 1246.0781666666667 to 1337.5862499999998\n",
      "Video /teamspace/studios/this_studio/100anime/81.mp4: 34581 frames at 23.976023976023978 FPS = 1442.315875 seconds\n",
      "Video 81 length: 1442.315875\n",
      "Intro: 114.489375 to 204.2457083333333\n",
      "Outro: 1337.5028333333332 to 1425.4657083333332\n",
      "Video /teamspace/studios/this_studio/100anime/82.mp4: 35534 frames at 23.976023976023978 FPS = 1482.0639166666665 seconds\n",
      "Video 82 length: 1482.0639166666665\n",
      "Intro: 1.5432083333333333 to 92.00858333333332\n",
      "Outro: 1353.477125 to 1451.1580416666666\n",
      "Video /teamspace/studios/this_studio/100anime/83.mp4: 33320 frames at 23.976023976023978 FPS = 1389.7216666666666 seconds\n",
      "Video 83 length: 1389.7216666666666\n",
      "Intro: 118.82704166666666 to 208.16629166666667\n",
      "Outro: 1265.0554583333333 to 1353.8525\n",
      "Video /teamspace/studios/this_studio/100anime/84.mp4: 34634 frames at 23.976023976023978 FPS = 1444.5264166666666 seconds\n",
      "Video 84 length: 1444.5264166666666\n",
      "Intro: 15.682333333333332 to 104.68791666666667\n",
      "Outro: 1308.8492083333333 to 1399.523125\n",
      "Found clips - Intro: 823, Regular: 570, Outro: 776\n",
      "Created 570 clips\n",
      "Initializing dataset with video_dir: D:/CS/ML/100anime, json_dir: D:/CS/ML/animev2/data/100 anime\n",
      "Found 0 video files\n",
      "Total videos: 0, Train idx: 0, Val idx: 0\n",
      "Loaded 0 annotations\n",
      "Found clips - Intro: 0, Regular: 0, Outro: 0\n",
      "Created 0 clips\n",
      "Dataset size: 0\n",
      "Dataset error: list index out of range\n",
      "Frames shape: torch.Size([16, 120, 3, 160, 160])\n",
      "Labels shape: torch.Size([16, 120, 3])\n",
      "Labels min/max: 0.0, 1.0\n",
      "Labels distribution:\n",
      "tensor([[3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [3., 8., 5.],\n",
      "        [2., 9., 5.],\n",
      "        [2., 9., 5.],\n",
      "        [2., 9., 5.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m model \u001b[38;5;241m=\u001b[39m AnimeClassifier()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# history = train_model(model, train_loader, val_loader)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m plot_training_history(\u001b[43mhistory\u001b[49m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Example inference on a single video\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# video_path = 'path/to/test_video.mp4'\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# json_path = 'path/to/test_video.json'\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# predictions, ground_truth, times = predict_video(model, video_path, json_path)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# plot_predictions(predictions, ground_truth, times)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AnimeDataset('/teamspace/studios/this_studio/100anime', '/teamspace/studios/this_studio/100 anime', mode='train')\n",
    "val_dataset = AnimeDataset('/teamspace/studios/this_studio/100anime', '/teamspace/studios/this_studio/100 anime', mode='val')\n",
    "\n",
    "    # Test dataset first\n",
    "try:\n",
    "    test_dataset = AnimeDataset('D:/CS/ML/100anime', 'D:/CS/ML/animev2/data/100 anime', mode='train')\n",
    "\n",
    "    print(f\"Dataset size: {len(test_dataset)}\")\n",
    "    # Test single item loading\n",
    "    frames, labels = test_dataset[0]\n",
    "    print(f\"Sample shapes - Frames: {frames.shape}, Labels: {labels.shape}\")\n",
    "    \n",
    "    verify_labels(test_dataset)\n",
    "except Exception as e:\n",
    "    print(f\"Dataset error: {str(e)}\")\n",
    "\n",
    "    # Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                            num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    " # Add this debug code before training\n",
    "for batch_idx, (frames, labels) in enumerate(train_loader):\n",
    "    if batch_idx == 0:  # Check first batch\n",
    "        print(f\"Frames shape: {frames.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Labels min/max: {labels.min()}, {labels.max()}\")\n",
    "        print(f\"Labels distribution:\\n{labels.sum(dim=0)}\")  # Sum for each class\n",
    "        break                         \n",
    "    \n",
    "    # Create and train model\n",
    "model = AnimeClassifier()\n",
    "# history = train_model(model, train_loader, val_loader)\n",
    "    \n",
    "    # Plot training history\n",
    "plot_training_history(history)\n",
    "    \n",
    "    # Example inference on a single video\n",
    "# video_path = 'path/to/test_video.mp4'\n",
    "# json_path = 'path/to/test_video.json'\n",
    "# predictions, ground_truth, times = predict_video(model, video_path, json_path)\n",
    "# plot_predictions(predictions, ground_truth, times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
