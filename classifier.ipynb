{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video partitionning using mobilenetV2 CNN and Bi-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SIZE = 160  # Resize frames to 160x160\n",
    "FPS = 4  # Sample 8 frames per second\n",
    "SEQUENCE_LENGTH = 120  # 30 seconds * 4 fps\n",
    "BATCH_SIZE = 16  # Will be adjusted based on memory\n",
    "NUM_CLASSES = 3  # intro, regular, outro\n",
    "FEATURE_DIM = 512  # Reduced feature dimension\n",
    "HIDDEN_DIM = 512  # LSTM hidden dimension\n",
    "ACCUMULATION_STEPS = 4  # Effective batch size of 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, video_dir, json_dir, clip_length=30, mode='train', train_split=0.7, val_split=0.15):\n",
    "        \"\"\"\n",
    "        Dataset for loading anime episodes and their annotations.\n",
    "        \n",
    "        Args:\n",
    "            video_dir: Directory containing video files\n",
    "            json_dir: Directory containing annotation JSON files\n",
    "            clip_length: Length of clips in seconds\n",
    "            mode: 'train', 'val', or 'test'\n",
    "            train_split: Proportion of data for training\n",
    "            val_split: Proportion of data for validation\n",
    "        \"\"\"\n",
    "        self.video_dir = Path(video_dir)\n",
    "        self.json_dir = Path(json_dir)\n",
    "        self.clip_length = clip_length\n",
    "        self.frame_interval = 1.0 / FPS\n",
    "        \n",
    "        # Get all video files\n",
    "        self.video_files = sorted(list(self.video_dir.glob('*.mp4')))\n",
    "        \n",
    "        # Split data\n",
    "        n_videos = len(self.video_files)\n",
    "        train_idx = int(n_videos * train_split)\n",
    "        val_idx = int(n_videos * (train_split + val_split))\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.video_files = self.video_files[:train_idx]\n",
    "        elif mode == 'val':\n",
    "            self.video_files = self.video_files[train_idx:val_idx]\n",
    "        else:  # test\n",
    "            self.video_files = self.video_files[val_idx:]\n",
    "            \n",
    "        # Load all annotations\n",
    "        self.annotations = {}\n",
    "        for video_file in self.video_files:\n",
    "            json_file = self.json_dir / f\"{video_file.stem}.json\"\n",
    "            with open(json_file, 'r') as f:\n",
    "                self.annotations[video_file.stem] = json.load(f)\n",
    "                \n",
    "        # Create clips with balanced sampling\n",
    "        self.clips = self._create_clips()\n",
    "        \n",
    "        # Image transforms\n",
    "        # self.transform = transforms.Compose([\n",
    "        #     transforms.ToTensor(),\n",
    "        #     transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "        #     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                       std=[0.229, 0.224, 0.225])\n",
    "        # ])\n",
    "         # Updated transforms with augmentation\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "            transforms.RandomHorizontalFlip(p=0.3),  # New\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),  # New\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ]) if mode == 'train' else transforms.Compose([  # Different transform for val/test\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def _create_clips(self):\n",
    "        \"\"\"\n",
    "        Create clips with balanced sampling between intro/regular/outro content.\n",
    "        Returns list of (video_path, start_time) tuples.\n",
    "        \"\"\"\n",
    "        clips = []\n",
    "        intro_clips = []\n",
    "        regular_clips = []\n",
    "        outro_clips = []\n",
    "        \n",
    "        for video_file in self.video_files:\n",
    "            ann = self.annotations[video_file.stem]\n",
    "            video_length = self._get_video_length(video_file)\n",
    "            \n",
    "            # Add intro clips\n",
    "            if 'intro_start' in ann and 'intro_end' in ann:\n",
    "                intro_start = ann['intro_start']\n",
    "                intro_end = ann['intro_end']\n",
    "                # Sample clips that contain intro\n",
    "                possible_starts = np.arange(\n",
    "                    max(0, intro_start - self.clip_length + 30),\n",
    "                    min(intro_end, video_length - self.clip_length),\n",
    "                    30  # Sample every 30 seconds\n",
    "                )\n",
    "                for start in possible_starts:\n",
    "                    intro_clips.append((video_file, start))\n",
    "            \n",
    "            # Add outro clips\n",
    "            if 'outro_start' in ann and 'outro_end' in ann:\n",
    "                outro_start = ann['outro_start']\n",
    "                outro_end = ann['outro_end']\n",
    "                possible_starts = np.arange(\n",
    "                    max(0, outro_start - self.clip_length + 30),\n",
    "                    min(outro_end, video_length - self.clip_length),\n",
    "                    30\n",
    "                )\n",
    "                for start in possible_starts:\n",
    "                    outro_clips.append((video_file, start))\n",
    "            \n",
    "            # Add regular clips\n",
    "            regular_regions = self._get_regular_regions(ann, video_length)\n",
    "            for start, end in regular_regions:\n",
    "                possible_starts = np.arange(\n",
    "                    start,\n",
    "                    end - self.clip_length,\n",
    "                    self.clip_length\n",
    "                )\n",
    "                for start in possible_starts:\n",
    "                    regular_clips.append((video_file, start))\n",
    "        \n",
    "        # Balance dataset by upsampling intro and outro clips\n",
    "        n_regular = len(regular_clips)\n",
    "        intro_clips = intro_clips * (n_regular // len(intro_clips) + 1)\n",
    "        outro_clips = outro_clips * (n_regular // len(outro_clips) + 1)\n",
    "        \n",
    "        # Combine and shuffle\n",
    "        clips = intro_clips[:n_regular//3] + outro_clips[:n_regular//3] + regular_clips[:n_regular//3]\n",
    "        random.shuffle(clips)\n",
    "        return clips\n",
    "    \n",
    "    def _get_video_length(self, video_path):\n",
    "        \"\"\"Get video length in seconds.\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "        return frame_count / fps\n",
    "    \n",
    "    def _get_regular_regions(self, ann, video_length):\n",
    "        \"\"\"Get time regions that are neither intro nor outro.\"\"\"\n",
    "        regions = []\n",
    "        intro_end = ann.get('intro_end', 0)\n",
    "        outro_start = ann.get('outro_start', video_length)\n",
    "        \n",
    "        if intro_end < outro_start:\n",
    "            regions.append((intro_end, outro_start))\n",
    "            \n",
    "        return regions\n",
    "    \n",
    "    def _get_frame_labels(self, video_file, start_time, n_frames):\n",
    "        \"\"\"Generate frame-level labels.\"\"\"\n",
    "        ann = self.annotations[video_file.stem]\n",
    "        times = np.linspace(start_time, start_time + self.clip_length, n_frames)\n",
    "        labels = np.zeros((n_frames, NUM_CLASSES))\n",
    "        \n",
    "        for i, t in enumerate(times):\n",
    "            if ann['intro_start'] <= t <= ann['intro_end']:\n",
    "                labels[i, 0] = 1  # intro\n",
    "            elif ann.get('outro_start', float('inf')) <= t <= ann.get('outro_end', float('inf')):\n",
    "                labels[i, 2] = 1  # outro\n",
    "            else:\n",
    "                labels[i, 1] = 1  # regular\n",
    "                \n",
    "        return labels\n",
    "    \n",
    "    def _sample_frames(self, video_path, start_time):\n",
    "        \"\"\"Sample frames from video at specified FPS.\"\"\"\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        \n",
    "        # Set starting position\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)\n",
    "        \n",
    "        # Sample frames\n",
    "        for _ in range(SEQUENCE_LENGTH):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Convert BGR to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "            \n",
    "            # Skip frames to maintain desired FPS\n",
    "            for _ in range(int(cap.get(cv2.CAP_PROP_FPS) / FPS) - 1):\n",
    "                cap.read()\n",
    "                \n",
    "        cap.release()\n",
    "        \n",
    "        # Pad sequence if needed\n",
    "        while len(frames) < SEQUENCE_LENGTH:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "            \n",
    "        return torch.stack(frames)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.clips)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_file, start_time = self.clips[idx]\n",
    "        frames = self._sample_frames(video_file, start_time)\n",
    "        labels = self._get_frame_labels(video_file, start_time, SEQUENCE_LENGTH)\n",
    "        return frames, torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model defenition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class AnimeClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # Feature extraction using MobileNetV2 (still efficient but full version)\n",
    "#         mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        \n",
    "#         # Adjust batch norm momentum for better training\n",
    "#         for m in mobilenet.modules():\n",
    "#             if isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.momentum = 0.1\n",
    "#                 m.eps = 1e-3\n",
    "#         self.features = nn.Sequential(*list(mobilenet.children())[:-1])\n",
    "        \n",
    "#         # Feature dimension reduction\n",
    "#         self.feature_reduction = nn.Sequential(\n",
    "#             nn.AdaptiveAvgPool2d(1),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(1280, FEATURE_DIM),\n",
    "#             nn.BatchNorm1d(FEATURE_DIM),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3)\n",
    "#         )\n",
    "        \n",
    "#         # Bidirectional LSTM layers\n",
    "#         self.lstm1 = nn.LSTM(FEATURE_DIM, HIDDEN_DIM, bidirectional=True, \n",
    "#                             batch_first=True, num_layers=2, dropout=0.3)\n",
    "#         self.lstm2 = nn.LSTM(HIDDEN_DIM * 2, HIDDEN_DIM, bidirectional=True, \n",
    "#                             batch_first=True, num_layers=2, dropout=0.3)\n",
    "        \n",
    "#         # Residual connection\n",
    "#         self.residual_projection = nn.Sequential(\n",
    "#             nn.Linear(FEATURE_DIM, HIDDEN_DIM * 2),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "        \n",
    "#         # Classification head (outputs logits for BCE with logits)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(HIDDEN_DIM * 2, HIDDEN_DIM),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(HIDDEN_DIM, NUM_CLASSES)\n",
    "#         )\n",
    "#         self._initialize_weights()\n",
    "\n",
    "#     def _initialize_weights(self):\n",
    "#         # Initialize LSTM weights\n",
    "#         for lstm in [self.lstm1, self.lstm2]:\n",
    "#             for name, param in lstm.named_parameters():\n",
    "#                 if 'weight_ih' in name:\n",
    "#                     nn.init.xavier_uniform_(param.data)\n",
    "#                 elif 'weight_hh' in name:\n",
    "#                     nn.init.orthogonal_(param.data)\n",
    "#                 elif 'bias' in name:\n",
    "#                     param.data.fill_(0)\n",
    "        \n",
    "#         # Initialize linear layers\n",
    "#         for m in self.feature_reduction.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "#         for m in self.residual_projection.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "#         for m in self.classifier.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         batch_size, seq_len, c, h, w = x.size()\n",
    "        \n",
    "#         # Process each frame through CNN\n",
    "#         x = x.view(batch_size * seq_len, c, h, w)\n",
    "#         features = self.features(x)\n",
    "#         features = self.feature_reduction(features)\n",
    "#         features = features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "#         # Save for residual connection\n",
    "#         residual = self.residual_projection(features)\n",
    "        \n",
    "#         # Process through LSTM layers\n",
    "#         lstm_out1, _ = self.lstm1(features)\n",
    "#         lstm_out2, _ = self.lstm2(lstm_out1)\n",
    "        \n",
    "#         # Add residual connection\n",
    "#         lstm_out = lstm_out2 + residual\n",
    "        \n",
    "#         # Classification\n",
    "#         return self.classifier(lstm_out)  # Returns logits\n",
    "class AnimeClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use MobileNetV2 but freeze early layers\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        # Freeze first 10 layers\n",
    "        for i, param in enumerate(mobilenet.features[:5].parameters()):\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.features = nn.Sequential(*list(mobilenet.children())[:-1])\n",
    "        \n",
    "        # Wider feature reduction\n",
    "        self.feature_reduction = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, FEATURE_DIM),\n",
    "            nn.LayerNorm(FEATURE_DIM),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(FEATURE_DIM, FEATURE_DIM),\n",
    "            nn.LayerNorm(FEATURE_DIM),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Single LSTM layer with higher capacity\n",
    "        self.lstm = nn.LSTM(FEATURE_DIM, HIDDEN_DIM, \n",
    "                           bidirectional=True, \n",
    "                           batch_first=True,\n",
    "                           num_layers=2,  # Single layer but higher dimension\n",
    "                           dropout=0.2)\n",
    "        \n",
    "        # # Simplified classification head\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Linear(HIDDEN_DIM * 2, HIDDEN_DIM),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(0.2),\n",
    "        #     nn.Linear(HIDDEN_DIM, NUM_CLASSES)\n",
    "        # )\n",
    "        # Wider classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_DIM * 2, HIDDEN_DIM * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(HIDDEN_DIM * 2, HIDDEN_DIM),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(HIDDEN_DIM, NUM_CLASSES)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        \n",
    "        # Process each frame through CNN\n",
    "        x = x.view(batch_size * seq_len, c, h, w)\n",
    "        features = self.features(x)\n",
    "        features = self.feature_reduction(features)\n",
    "        features = features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Process through LSTM layer\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(lstm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function + training epoch + validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_consistency_loss(predictions, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Calculate temporal consistency loss to penalize rapid changes in predictions.\n",
    "    \"\"\"\n",
    "    temp_loss = torch.mean(torch.abs(predictions[:, 1:] - predictions[:, :-1]))\n",
    "    return alpha * temp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc='Training')\n",
    "    \n",
    "    for frames, labels in progress_bar:\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            logits = model(frames)\n",
    "            loss = criterion(torch.sigmoid(logits), labels)  # Apply sigmoid here\n",
    "            temp_loss = temporal_consistency_loss(torch.sigmoid(logits))\n",
    "            loss = loss + temp_loss\n",
    "        \n",
    "        # scaler.scale(loss).backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        # Add gradient clipping\n",
    "        if scaler.is_enabled():  # Check if we're using mixed precision\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(val_loader, desc='Validation'):\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "            \n",
    "            predictions = model(frames)\n",
    "            bce_loss = criterion(predictions, labels)\n",
    "            temp_loss = temporal_consistency_loss(predictions)\n",
    "            loss = bce_loss + temp_loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.append(predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, 0)\n",
    "    all_labels = torch.cat(all_labels, 0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = ((all_preds.argmax(dim=-1) == all_labels.argmax(dim=-1)).float().mean()).item()\n",
    "    \n",
    "    return total_loss / len(val_loader), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=15, patience=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Use BCEWithLogitsLoss instead of BCELoss for better numeric stability\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-3, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "   # Better learning rate scheduling with warmup\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=2e-2,  # Peak learning rate\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.2,  # 10% warmup\n",
    "        div_factor=10,  # Initial lr = max_lr/10\n",
    "        final_div_factor=100,  # Final lr = initial_lr/100\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'epoch_times': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    best_model_path = f'best_model_{current_time}.pth'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "            val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "            \n",
    "            scheduler.step()\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            # Save periodic checkpoint (every 5 epochs)\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                checkpoint_path = f'checkpoint_epoch_{epoch+1}_{current_time}.pth'\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'history': history\n",
    "                }, checkpoint_path)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                }, best_model_path)\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "\n",
    "\n",
    "             # Record history\n",
    "            epoch_time = time.time() - start_time\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_accuracy'].append(val_accuracy)\n",
    "            history['epoch_times'].append(epoch_time)\n",
    "            history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "            print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.4f}, LR: {current_lr:.6f}')\n",
    "            print(f'Epoch time: {epoch_time:.2f}s')\n",
    "            \n",
    "                \n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'\\nEarly stopping after {epoch + 1} epochs')\n",
    "                break\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "        \n",
    "    finally:\n",
    "        # Always try to save final state\n",
    "        try:\n",
    "            final_path = f'final_model_{current_time}.pth'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'history': history\n",
    "            }, final_path)\n",
    "            print(f\"\\nFinal model saved to {final_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving final model: {str(e)}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training metrics over time.\n",
    "    Shows loss curves and validation accuracy.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Validation Accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot epoch times\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['epoch_times'], label='Epoch Time')\n",
    "    plt.title('Training Time per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(model, video_path, json_path=None, window_size=60, overlap=30):\n",
    "    \"\"\"\n",
    "    Predict intro/outro segments for an entire video using overlapping windows.\n",
    "    Returns frame-level predictions and optionally compares with ground truth.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = 'cpu'\n",
    "    model.eval()\n",
    "    \n",
    "    # Video properties\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / video_fps\n",
    "    cap.release()\n",
    "    \n",
    "    # Calculate number of windows with overlap\n",
    "    stride = window_size - overlap\n",
    "    n_windows = int(np.ceil((duration - window_size) / stride)) + 1\n",
    "    \n",
    "    # Initialize arrays for predictions\n",
    "    all_predictions = np.zeros((int(duration * FPS), NUM_CLASSES))\n",
    "    counts = np.zeros(int(duration * FPS))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((FRAME_SIZE, FRAME_SIZE)),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Process each window\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(n_windows), desc='Processing video'):\n",
    "            start_time = i * stride\n",
    "            \n",
    "            # Sample frames for this window\n",
    "            cap = cv2.VideoCapture(str(video_path))\n",
    "            cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)\n",
    "            \n",
    "            frames = []\n",
    "            for _ in range(SEQUENCE_LENGTH):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = transform(frame)\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "            \n",
    "            # If we don't have enough frames, pad with zeros\n",
    "            while len(frames) < SEQUENCE_LENGTH:\n",
    "                frames.append(torch.zeros_like(frames[0]))\n",
    "            \n",
    "            # Make prediction\n",
    "            frames_tensor = torch.stack(frames).unsqueeze(0).to(device)\n",
    "            predictions = model(frames_tensor).cpu().numpy()[0]\n",
    "            \n",
    "            # Add predictions to the appropriate time slots\n",
    "            start_idx = int(start_time * FPS)\n",
    "            end_idx = start_idx + predictions.shape[0]\n",
    "            all_predictions[start_idx:end_idx] += predictions\n",
    "            counts[start_idx:end_idx] += 1\n",
    "    \n",
    "    # Average overlapping predictions\n",
    "    mask = counts > 0\n",
    "    all_predictions[mask] /= counts[mask, np.newaxis]\n",
    "    \n",
    "    # Load ground truth if available\n",
    "    if json_path:\n",
    "        with open(json_path, 'r') as f:\n",
    "            gt = json.load(f)\n",
    "            \n",
    "        gt_labels = np.zeros((int(duration * FPS), NUM_CLASSES))\n",
    "        times = np.arange(0, duration, 1/FPS)\n",
    "        \n",
    "        for i, t in enumerate(times):\n",
    "            if gt['intro_start'] <= t <= gt['intro_end']:\n",
    "                gt_labels[i, 0] = 1\n",
    "            elif gt.get('outro_start', float('inf')) <= t <= gt.get('outro_end', float('inf')):\n",
    "                gt_labels[i, 2] = 1\n",
    "            else:\n",
    "                gt_labels[i, 1] = 1\n",
    "    else:\n",
    "        gt_labels = None\n",
    "    \n",
    "    return all_predictions, gt_labels, times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizes model predictions against ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(predictions, ground_truth, times):\n",
    "    \"\"\"\n",
    "    Visualize predictions against ground truth if available.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.plot(times, predictions[:, 0], 'b-', label='Intro Pred', alpha=0.7)\n",
    "    plt.plot(times, predictions[:, 2], 'r-', label='Outro Pred', alpha=0.7)\n",
    "    \n",
    "    if ground_truth is not None:\n",
    "        # Plot ground truth\n",
    "        plt.plot(times, ground_truth[:, 0], 'b--', label='Intro GT')\n",
    "        plt.plot(times, ground_truth[:, 2], 'r--', label='Outro GT')\n",
    "    \n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Intro/Outro Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset error: integer division or modulo by zero\n",
      "Frames shape: torch.Size([16, 120, 3, 160, 160])\n",
      "Labels shape: torch.Size([16, 120, 3])\n",
      "Labels min/max: 0.0, 1.0\n",
      "Labels distribution:\n",
      "tensor([[ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 5.,  7.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 4.,  8.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3.,  9.,  4.],\n",
      "        [ 3., 10.,  3.],\n",
      "        [ 3., 10.,  3.],\n",
      "        [ 3., 10.,  3.],\n",
      "        [ 3., 10.,  3.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m AnimeClassifier()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# history = train_model(model, train_loader, val_loader)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m plot_training_history(\u001b[43mhistory\u001b[49m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Example inference on a single video\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# video_path = 'path/to/test_video.mp4'\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# json_path = 'path/to/test_video.json'\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# predictions, ground_truth, times = predict_video(model, video_path, json_path)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# plot_predictions(predictions, ground_truth, times)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AnimeDataset('/teamspace/studios/this_studio/100anime', '/teamspace/studios/this_studio/100 anime', mode='train')\n",
    "val_dataset = AnimeDataset('/teamspace/studios/this_studio/100anime', '/teamspace/studios/this_studio/100 anime', mode='val')\n",
    "\n",
    "    # Test dataset first\n",
    "try:\n",
    "    test_dataset = AnimeDataset('D:/CS/ML/100anime', 'D:/CS/ML/animev2/data/100 anime', mode='train')\n",
    "    print(f\"Dataset size: {len(test_dataset)}\")\n",
    "    # Test single item loading\n",
    "    frames, labels = test_dataset[0]\n",
    "    print(f\"Sample shapes - Frames: {frames.shape}, Labels: {labels.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Dataset error: {str(e)}\")\n",
    "\n",
    "    # Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                            num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    " # Add this debug code before training\n",
    "for batch_idx, (frames, labels) in enumerate(train_loader):\n",
    "    if batch_idx == 0:  # Check first batch\n",
    "        print(f\"Frames shape: {frames.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Labels min/max: {labels.min()}, {labels.max()}\")\n",
    "        print(f\"Labels distribution:\\n{labels.sum(dim=0)}\")  # Sum for each class\n",
    "        break                         \n",
    "    \n",
    "    # Create and train model\n",
    "model = AnimeClassifier()\n",
    "# history = train_model(model, train_loader, val_loader)\n",
    "    \n",
    "    # Plot training history\n",
    "plot_training_history(history)\n",
    "    \n",
    "    # Example inference on a single video\n",
    "# video_path = 'path/to/test_video.mp4'\n",
    "# json_path = 'path/to/test_video.json'\n",
    "# predictions, ground_truth, times = predict_video(model, video_path, json_path)\n",
    "# plot_predictions(predictions, ground_truth, times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
