{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "CONFIG = {\n",
    "    # Data Parameters\n",
    "    'FRAME_SIZE': 224,          # Decrease if GPU memory is issue, increase if underfitting\n",
    "    'FPS': 8,                   # Frames per second to sample\n",
    "    'SEQUENCE_LENGTH': 240,     # 30 seconds * 8 fps\n",
    "    'NUM_CLASSES': 3,           # intro, regular, outro\n",
    "    \n",
    "    # Model Architecture\n",
    "    'HIDDEN_DIM': 256,          # GRU hidden dimension - increase if underfitting\n",
    "    'NUM_LAYERS': 2,            # GRU layers - increase for more complexity\n",
    "    'DROPOUT': 0.3,             # Increase if overfitting, decrease if underfitting\n",
    "    \n",
    "    # Training Parameters\n",
    "    'BATCH_SIZE': 2,            # Decrease if GPU OOM, increase if GPU underutilized\n",
    "    'ACCUMULATION_STEPS': 4,    # Effective batch size = BATCH_SIZE * ACCUMULATION_STEPS\n",
    "    'NUM_EPOCHS': 30,           # Maximum epochs to train\n",
    "    'PATIENCE': 5,              # Early stopping patience\n",
    "    'LEARNING_RATE': 1e-4,      # Decrease if training unstable\n",
    "    'WEIGHT_DECAY': 1e-2,       # L2 regularization - increase if overfitting\n",
    "    'NUM_WORKERS': 4,           # Data loading workers - set to CPU count\n",
    "    \n",
    "    # Scheduler Parameters\n",
    "    'WARMUP_EPOCHS': 2,         # Linear warmup epochs\n",
    "    'MIN_LR': 1e-6,            # Minimum learning rate\n",
    "    \n",
    "    # Validation\n",
    "    'VAL_FREQ': 1,             # Validate every N epochs\n",
    "    'SAVE_FREQ': 5,            # Save checkpoint every N epochs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeFrameDataset(Dataset):\n",
    "    def __init__(self, video_dir: str, json_dir: str, \n",
    "                 clip_length: int = 30, fps: int = 8, \n",
    "                 split: str = 'train', seed: int = 42):\n",
    "        \"\"\"Same docstring as before\"\"\"\n",
    "        self.video_dir = Path(video_dir)\n",
    "        self.json_dir = Path(json_dir)\n",
    "        self.clip_length = clip_length\n",
    "        self.fps = fps\n",
    "        self.frames_per_clip = clip_length * fps\n",
    "        \n",
    "        random.seed(seed)\n",
    "        \n",
    "        # Get sorted list of all video files\n",
    "        self.video_files = sorted(list(self.video_dir.glob('*.mp4')))\n",
    "        \n",
    "        # Split data\n",
    "        random.shuffle(self.video_files)\n",
    "        n_videos = len(self.video_files)\n",
    "        train_idx = int(0.8 * n_videos)\n",
    "        val_idx = int(0.9 * n_videos)\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.video_files = self.video_files[:train_idx]\n",
    "        elif split == 'val':\n",
    "            self.video_files = self.video_files[train_idx:val_idx]\n",
    "        else:  # test\n",
    "            self.video_files = self.video_files[val_idx:]\n",
    "            \n",
    "        # Load all annotations\n",
    "        self.annotations = {}\n",
    "        for video_file in self.video_files:\n",
    "            json_file = self.json_dir / f\"{video_file.stem}.json\"\n",
    "            if not json_file.exists():\n",
    "                print(f\"Warning: Missing JSON for {video_file.stem}\")\n",
    "                continue\n",
    "            with open(json_file, 'r') as f:\n",
    "                self.annotations[video_file.stem] = json.load(f)\n",
    "        \n",
    "        # Setup transforms first so _create_balanced_clips can use it\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Create balanced clips with debug info\n",
    "        print(f\"Creating clips for {len(self.video_files)} videos...\")\n",
    "        self.clips = self._create_balanced_clips()\n",
    "        \n",
    "    def _create_balanced_clips(self) -> List[Tuple[Path, float]]:\n",
    "        \"\"\"Creates balanced list of (video_path, start_time) clips\"\"\"\n",
    "        intro_clips = []\n",
    "        outro_clips = []\n",
    "        regular_clips = []\n",
    "        transition_clips = []\n",
    "        \n",
    "        for video_file in tqdm(self.video_files, desc=\"Creating clips\"):\n",
    "            if video_file.stem not in self.annotations:\n",
    "                continue\n",
    "                \n",
    "            ann = self.annotations[video_file.stem]\n",
    "            try:\n",
    "                duration = self._get_video_duration(video_file)\n",
    "            except:\n",
    "                print(f\"Warning: Could not read duration for {video_file}\")\n",
    "                continue\n",
    "\n",
    "            # Debug print\n",
    "            print(f\"\\nProcessing {video_file.stem}:\")\n",
    "            print(f\"Duration: {duration:.2f}s\")\n",
    "            print(f\"Annotations: {ann}\")\n",
    "                \n",
    "            # Sample intro clips\n",
    "            if 'intro_start' in ann and 'intro_end' in ann:\n",
    "                start = ann['intro_start']\n",
    "                end = ann['intro_end']\n",
    "                if start > self.clip_length/2:\n",
    "                    transition_clips.append((video_file, max(0, start - self.clip_length/2)))\n",
    "                \n",
    "                # Sample from middle of intro with error handling\n",
    "                try:\n",
    "                    intro_times = np.arange(start + 5, end - self.clip_length, self.clip_length/2)\n",
    "                    intro_clips.extend((video_file, t) for t in intro_times)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Error sampling intro clips: {e}\")\n",
    "                \n",
    "                if end + self.clip_length/2 < duration:\n",
    "                    transition_clips.append((video_file, end - self.clip_length/2))\n",
    "            \n",
    "            # Sample outro clips\n",
    "            if 'outro_start' in ann and 'outro_end' in ann:\n",
    "                start = ann['outro_start']\n",
    "                end = min(ann['outro_end'], duration)  # Ensure we don't exceed duration\n",
    "                if start > self.clip_length/2:\n",
    "                    transition_clips.append((video_file, max(0, start - self.clip_length/2)))\n",
    "                \n",
    "                try:\n",
    "                    outro_times = np.arange(start + 5, end - self.clip_length, self.clip_length/2)\n",
    "                    outro_clips.extend((video_file, t) for t in outro_times)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Error sampling outro clips: {e}\")\n",
    "            \n",
    "            # Sample regular clips\n",
    "            regular_start = ann.get('intro_end', 0)\n",
    "            regular_end = ann.get('outro_start', duration)\n",
    "            if regular_end > regular_start:\n",
    "                try:\n",
    "                    regular_times = np.arange(regular_start + 5, regular_end - self.clip_length, \n",
    "                                           self.clip_length * 2)\n",
    "                    regular_clips.extend((video_file, t) for t in regular_times)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Error sampling regular clips: {e}\")\n",
    "\n",
    "        # Print statistics before balancing\n",
    "        print(\"\\nBefore balancing:\")\n",
    "        print(f\"Intro clips: {len(intro_clips)}\")\n",
    "        print(f\"Outro clips: {len(outro_clips)}\")\n",
    "        print(f\"Regular clips: {len(regular_clips)}\")\n",
    "        print(f\"Transition clips: {len(transition_clips)}\")\n",
    "        \n",
    "        # Balance dataset with minimum available clips\n",
    "        min_clips = min(\n",
    "            max(len(intro_clips) // 2, 1),\n",
    "            max(len(outro_clips) // 2, 1),\n",
    "            max(len(transition_clips), 1)\n",
    "        )\n",
    "        \n",
    "        balanced_clips = (\n",
    "            random.sample(intro_clips, min(len(intro_clips), min_clips * 2)) +\n",
    "            random.sample(outro_clips, min(len(outro_clips), min_clips * 2)) +\n",
    "            random.sample(transition_clips, min(len(transition_clips), min_clips)) +\n",
    "            random.sample(regular_clips, min(len(regular_clips), min_clips * 4))\n",
    "        )\n",
    "        \n",
    "        # Print final statistics\n",
    "        print(\"\\nAfter balancing:\")\n",
    "        print(f\"Total clips: {len(balanced_clips)}\")\n",
    "        \n",
    "        random.shuffle(balanced_clips)\n",
    "        return balanced_clips\n",
    "    \n",
    "    def _get_video_duration(self, video_path: Path) -> float:\n",
    "        \"\"\"Get video duration in seconds\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        cap.release()\n",
    "        return frame_count / fps\n",
    "    \n",
    "    def _load_frames(self, video_path: Path, start_time: float) -> torch.Tensor:\n",
    "        \"\"\"Load sequence of frames from video\"\"\"\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        \n",
    "        # Set starting position\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)\n",
    "        \n",
    "        for _ in range(self.frames_per_clip):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "            \n",
    "            # Skip frames to maintain desired FPS\n",
    "            for _ in range(int(cap.get(cv2.CAP_PROP_FPS) / self.fps) - 1):\n",
    "                cap.read()\n",
    "                \n",
    "        cap.release()\n",
    "        \n",
    "        # Pad sequence if needed\n",
    "        while len(frames) < self.frames_per_clip:\n",
    "            frames.append(torch.zeros_like(frames[0]))\n",
    "            \n",
    "        return torch.stack(frames)\n",
    "    \n",
    "    def _get_labels(self, video_file: Path, start_time: float) -> torch.Tensor:\n",
    "        \"\"\"Generate frame-level labels\"\"\"\n",
    "        ann = self.annotations[video_file.stem]\n",
    "        times = np.linspace(start_time, \n",
    "                          start_time + self.clip_length, \n",
    "                          self.frames_per_clip)\n",
    "        labels = torch.zeros((self.frames_per_clip, 3))\n",
    "        \n",
    "        for i, t in enumerate(times):\n",
    "            if ann.get('intro_start', float('inf')) <= t <= ann.get('intro_end', -float('inf')):\n",
    "                labels[i, 0] = 1  # intro\n",
    "            elif ann.get('outro_start', float('inf')) <= t <= ann.get('outro_end', -float('inf')):\n",
    "                labels[i, 2] = 1  # outro\n",
    "            else:\n",
    "                labels[i, 1] = 1  # regular content\n",
    "                \n",
    "        return labels\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.clips)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        video_file, start_time = self.clips[idx]\n",
    "        frames = self._load_frames(video_file, start_time)\n",
    "        labels = self._get_labels(video_file, start_time)\n",
    "        return frames, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clip(dataset: AnimeFrameDataset, idx: int = 1021) -> None:\n",
    "    \"\"\"Visualize frames from a random or specified clip with labels\"\"\"\n",
    "    if idx is None:\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "    \n",
    "    frames, labels = dataset[idx]\n",
    "    video_file, start_time = dataset.clips[idx]\n",
    "    \n",
    "    # Select 3 evenly spaced frames\n",
    "    n_frames = frames.shape[0]\n",
    "    frame_indices = [n_frames//4, n_frames//2, 3*n_frames//4]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, frame_idx in enumerate(frame_indices):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        \n",
    "        # Convert tensor to image\n",
    "        frame = frames[frame_idx].permute(1, 2, 0)\n",
    "        frame = frame * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
    "        frame = frame.clip(0, 1).numpy()\n",
    "        \n",
    "        plt.imshow(frame)\n",
    "        frame_time = start_time + frame_idx/dataset.fps\n",
    "        label_idx = labels[frame_idx].argmax().item()\n",
    "        label_name = ['Intro', 'Regular', 'Outro'][label_idx]\n",
    "        \n",
    "        plt.title(f'Time: {frame_time:.2f}s\\nLabel: {label_name}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Video: {video_file.stem}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset: AnimeFrameDataset) -> Dict:\n",
    "    \"\"\"Analyze dataset statistics\"\"\"\n",
    "    label_counts = torch.zeros(3)\n",
    "    clip_types = {'intro': 0, 'outro': 0, 'regular': 0, 'transition': 0}\n",
    "    \n",
    "    for frames, labels in tqdm(dataset, desc=\"Analyzing dataset\"):\n",
    "        label_indices = labels.argmax(dim=1)\n",
    "        unique_labels = torch.unique(label_indices)\n",
    "        \n",
    "        # Count frame labels\n",
    "        for i in range(3):\n",
    "            label_counts[i] += (label_indices == i).sum().item()\n",
    "            \n",
    "        # Categorize clip type\n",
    "        if len(unique_labels) > 1:\n",
    "            clip_types['transition'] += 1\n",
    "        elif 0 in unique_labels:\n",
    "            clip_types['intro'] += 1\n",
    "        elif 2 in unique_labels:\n",
    "            clip_types['outro'] += 1\n",
    "        else:\n",
    "            clip_types['regular'] += 1\n",
    "    \n",
    "    return {\n",
    "        'total_clips': len(dataset),\n",
    "        'total_frames': label_counts.sum().item(),\n",
    "        'label_distribution': {\n",
    "            'intro': label_counts[0].item(),\n",
    "            'regular': label_counts[1].item(),\n",
    "            'outro': label_counts[2].item()\n",
    "        },\n",
    "        'clip_types': clip_types\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(video_dir, json_dir):\n",
    "    \"\"\"Test dataset creation with specified paths\"\"\"\n",
    "    print(f\"Testing dataset creation...\")\n",
    "    print(f\"Video directory: {video_dir}\")\n",
    "    print(f\"JSON directory: {json_dir}\")\n",
    "    \n",
    "    try:\n",
    "        dataset = AnimeFrameDataset(\n",
    "            video_dir=video_dir,\n",
    "            json_dir=json_dir,\n",
    "            split='train'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDataset created successfully!\")\n",
    "        print(f\"Total clips: {len(dataset)}\")\n",
    "        \n",
    "        if len(dataset) > 0:\n",
    "            frames, labels = dataset[0]\n",
    "            print(f\"Sample shapes:\")\n",
    "            print(f\"- Frames: {frames.shape}\")\n",
    "            print(f\"- Labels: {labels.shape}\")\n",
    "            \n",
    "            # Test visualization\n",
    "            visualize_clip(dataset)\n",
    "            \n",
    "        return dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataset: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = test_dataset(\n",
    "    video_dir='/teamspace/studios/this_studio/100anime',\n",
    "    json_dir='/teamspace/studios/this_studio/100 anime'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeSceneClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # MobileNetV2 feature extractor\n",
    "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(mobilenet.children())[:-1])\n",
    "        \n",
    "        # Feature dimension reduction (from 1280 to hidden_dim)\n",
    "        self.reduce_dim = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Bidirectional GRU\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 3)  # 3 classes: intro, regular, outro\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        \n",
    "        # Extract features\n",
    "        x = x.view(batch_size * seq_len, c, h, w)\n",
    "        x = self.features(x)\n",
    "        x = self.reduce_dim(x)\n",
    "        \n",
    "        # Reshape for GRU\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Apply GRU\n",
    "        x, _ = self.gru(x)\n",
    "        \n",
    "        # Classify each timestep\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AnimeSceneClassifier().to(device)\n",
    "    \n",
    "    # Test with random input\n",
    "    x = torch.randn(2, 240, 3, 224, 224).to(device)  # batch_size=2, seq_len=240\n",
    "    y = model(x)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {y.shape}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=config['LEARNING_RATE'], weight_decay=config['WEIGHT_DECAY'])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # OneCycle scheduler\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=config['LEARNING_RATE'],\n",
    "        epochs=config['NUM_EPOCHS'],\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=config['WARMUP_EPOCHS'] / config['NUM_EPOCHS'],\n",
    "        final_div_factor=config['LEARNING_RATE'] / config['MIN_LR']\n",
    "    )\n",
    "    \n",
    "    # Initialize wandb\n",
    "    run = wandb.init(project=\"anime-scene-classifier\", config=config)\n",
    "    \n",
    "    # Training state\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Save directory\n",
    "    save_dir = f\"checkpoints_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(config['NUM_EPOCHS']):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for batch_idx, (frames, labels) in enumerate(tqdm(train_loader)):\n",
    "                frames, labels = frames.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass with mixed precision\n",
    "                with autocast():\n",
    "                    outputs = model(frames)\n",
    "                    loss = criterion(outputs, labels) / config['ACCUMULATION_STEPS']\n",
    "                \n",
    "                # Backward pass\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                if (batch_idx + 1) % config['ACCUMULATION_STEPS'] == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    scheduler.step()\n",
    "                \n",
    "                train_loss += loss.item() * config['ACCUMULATION_STEPS']\n",
    "                \n",
    "                # Log batch metrics\n",
    "                if batch_idx % 10 == 0:\n",
    "                    wandb.log({\n",
    "                        'batch_loss': loss.item() * config['ACCUMULATION_STEPS'],\n",
    "                        'learning_rate': scheduler.get_last_lr()[0]\n",
    "                    })\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            if epoch % config['VAL_FREQ'] == 0:\n",
    "                val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "                \n",
    "                wandb.log({\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_accuracy': val_acc\n",
    "                })\n",
    "                \n",
    "                print(f'Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}')\n",
    "                \n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(),\n",
    "                        'val_loss': val_loss,\n",
    "                    }, f'{save_dir}/best_model.pth')\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= config['PATIENCE']:\n",
    "                    print(f'Early stopping after {epoch} epochs')\n",
    "                    break\n",
    "            \n",
    "            # Regular checkpoints\n",
    "            if epoch % config['SAVE_FREQ'] == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                }, f'{save_dir}/checkpoint_epoch_{epoch}.pth')\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print('Training interrupted')\n",
    "    \n",
    "    finally:\n",
    "        run.finish()\n",
    "        torch.save(model.state_dict(), f'{save_dir}/final_model.pth')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, labels in val_loader:\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            pred = torch.argmax(outputs, dim=2)\n",
    "            true = torch.argmax(labels, dim=2)\n",
    "            correct += (pred == true).sum().item()\n",
    "            total += true.numel()\n",
    "    \n",
    "    return val_loss / len(val_loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnimeSceneClassifier()\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['BATCH_SIZE'],\n",
    "        shuffle=True,\n",
    "        num_workers=CONFIG['NUM_WORKERS'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['BATCH_SIZE'],\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG['NUM_WORKERS'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "trained_model = train_model(model, train_loader, val_loader, CONFIG)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
